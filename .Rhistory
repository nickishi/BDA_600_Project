#we can plot parameter performance
plot(c.knn)
#let preprocess K means performs better when standardized!
set.seed(195) #ALWAYS USE same SEED ACROSS trains to ensure identical cv folds
c.knn.pp <- train(Balance ~ ., data= credit.train, method = "knn",
preProcess=c("center", "scale"),
tuneGrid=k.grid, trControl=ctrl)
c.knn.pp <- train(Balance ~ ., data= df.train, method = "knn",
preProcess=c("center", "scale"),
tuneGrid=k.grid, trControl=ctrl)
c.knn.pp <- train(totalFare ~ ., data= df.train, method = "knn",
preProcess=c("center", "scale"),
tuneGrid=k.grid, trControl=ctrl)
c.knn.pp
plot(c.knn.pp)
##COMPARE MODEL PERFORMANCE
#let's compare all of the model resampling performance
#first lets put all trained models in a list object
c.models<- list("LM Base"=c.lm, "LM PreProc" = c.lm.pp,
"LM Cor Rmved" = c.lm.cor, "KNN Base"=c.knn,
"KNN PreProc" = c.knn.pp)
#review residuals and actual observations against predictions
#should review interesting models using graphs and summary
par(mfrow=c(1,2))
plot(credit.train$Balance ~ predict(c.lm), xlab="predict", ylab="actual")
plot(df.train$totalFare ~ predict(c.lm), xlab="predict", ylab="actual")
c.lm
setwd("~/Google Drive/MIS720/MIS720 Project")
library(tidyr)
library(ggplot2)
library(lattice)
itineraries <- read.csv("itineraries_subset.csv",stringsAsFactors = TRUE)
# define data types
data$legId <- as.character(data$legId)
data$searchDate <- as.Date(data$searchDate)
data$flightDate <- as.Date(data$flightDate)
# remove leading PT and transform hours to minutes
data$travelDuration <- gsub("^.{0,2}", "", data$travelDuration)
datasegmentsDistance<-NULL
data$X <- NULL
data$legId <- NULL
data$searchDate <- NULL
head(data)
# remove leading PT and transform hours to minutes
data$travelDuration <- gsub("^.{0,2}", "", data$travelDuration)
colnames(data)
# drop all NA values
data <- na.omit(itineraries)
colnames(data)
# define data types
data$legId <- as.character(data$legId)
data$searchDate <- as.Date(data$searchDate)
data$flightDate <- as.Date(data$flightDate)
# remove leading PT and transform hours to minutes
data$travelDuration <- gsub("^.{0,2}", "", data$travelDuration)
# split columns
data <- data %>% separate(travelDuration, into = c("travelDuration_hours", "travelDuration_mins"), sep="H")
# remove last character M
data$travelDuration_mins <- gsub("M","", as.character(itineraries$travelDuration_mins))
colnames(data)
# remove last character M
data$travelDuration_mins <- gsub("M","", as.character(itineraries$travelDuration_mins))
itineraries <- read.csv("itineraries_subset.csv")
# drop all NA values
data <- na.omit(itineraries)
dim(data)
# define data types
data$legId <- as.character(data$legId)
data$searchDate <- as.Date(data$searchDate)
data$flightDate <- as.Date(data$flightDate)
# remove leading PT and transform hours to minutes
data$travelDuration <- gsub("^.{0,2}", "", data$travelDuration)
# split columns
data <- data %>% separate(travelDuration, into = c("travelDuration_hours", "travelDuration_mins"), sep="H")
# remove last character M
data$travelDuration_mins <- gsub("M","", as.character(itineraries$travelDuration_mins))
# remove last character M
data$travelDuration_mins <- gsub("M","", as.character(data$travelDuration_mins))
# covert hours into minutes
data$travelDuration_hours_to_mins <- as.integer(data$travelDuration_hours) * 60
data$travelDuration_minutes <- data$travelDuration_hours_to_mins + as.integer(data$travelDuration_mins)
data_cleaned <- subset(data, select=-c(travelDuration_hours_to_mins, travelDuration_mins, travelDuration_hours, travelDuration))
data_cleaned <- subset(data, select=-c(travelDuration_hours_to_mins, travelDuration_mins, travelDuration_hours))
datasegmentsDistance<-NULL
data$X <- NULL
data$legId <- NULL
data$searchDate <- NULL
data$flightDate <- NULL
data$fareBasisCode <- NULL
data$baseFare <- NULL
data$segmentsDepartureTimeEpochSeconds <- NULL
data$segmentsArrivalAirportCode<-NULL
data$segmentsDistance <- NULL
data$segmentsDepartureTimeRaw <- NULL
data$segmentsArrivalTimeEpochSeconds <- NULL
data$segmentsArrivalTimeRaw <- NULL
data$segmentsDepartureAirportCode <- NULL
data$segmentsAirlineCode <- NULL
data$segmentsAirlineName <- NULL
data$segmentsEquipmentDescription <- NULL
data$segmentsDurationInSeconds <- NULL
colnames(data)
train.index <- sample(nrow(data.dmodel), nrow(data.dmodel) * .85) #let keep 85% of data for training
data.dmodel <- dummyVars( ~ ., data=data, fullRank=F)
set.seed(100)
train.index <- sample(nrow(data.dmodel), nrow(data.dmodel) * .85) #let keep 85% of data for training
?sample
?rample
??sample
train.index <- sample(nrow(data.dmodel), nrow(data.dmodel) * .85) #let keep 85% of data for training
length(data.dmodel)
data.dmodel <- dummyVars( ~ ., data=data, fullRank=F)
data.dmodel
data.dmodel <- as.data.frame(predict(data.dmodel, data))
set.seed(100)
train.index <- sample(nrow(data.dmodel), nrow(data.dmodel) * .85) #let keep 85% of data for training
#create test and traing data frames
Air.train <- data.dmodel [train.index,] #model with this
Air.test <- data.dmodel [-train.index,]#we don't touch this while training
library(rpart)
set.seed(100)
tree <- rpart(totalFare ~., date=Air.train)
tree <- rpart(totalFare ~ ., data=Air.train)
plot(tree)
opt.cp <- tree$cptable[which.min(tree$cptable[,"xerror"]),"CP"]
itineraries <- read.csv("itineraries_subset.csv")
# drop all NA values
data <- na.omit(itineraries)
# define data types
data$legId <- as.character(data$legId)
data$searchDate <- as.Date(data$searchDate)
data$flightDate <- as.Date(data$flightDate)
# remove leading PT and transform hours to minutes
data$travelDuration <- gsub("^.{0,2}", "", data$travelDuration)
# split columns
data <- data %>% separate(travelDuration, into = c("travelDuration_hours", "travelDuration_mins"), sep="H")
# remove last character M
data$travelDuration_mins <- gsub("M","", as.character(data$travelDuration_mins))
# covert hours into minutes
data$travelDuration_hours_to_mins <- as.integer(data$travelDuration_hours) * 60
data$travelDuration_minutes <- data$travelDuration_hours_to_mins + as.integer(data$travelDuration_mins)
data_cleaned <- subset(data, select=-c(travelDuration_hours_to_mins, travelDuration_mins, travelDuration_hours))
summary(data)
datasegmentsDistance<-NULL
data$X <- NULL
data$legId <- NULL
data$searchDate <- NULL
data$flightDate <- NULL
data$fareBasisCode <- NULL
data$baseFare <- NULL
data$segmentsDepartureTimeEpochSeconds <- NULL
data$segmentsArrivalAirportCode<-NULL
data$segmentsDistance <- NULL
data$segmentsDepartureTimeRaw <- NULL
data$segmentsArrivalTimeEpochSeconds <- NULL
data$segmentsArrivalTimeRaw <- NULL
data$segmentsDepartureAirportCode <- NULL
data$segmentsAirlineCode <- NULL
data$segmentsAirlineName <- NULL
data$segmentsEquipmentDescription <- NULL
data$segmentsDurationInSeconds <- NULL
View(data)
data.dmodel <- dummyVars( ~ ., data=data, fullRank=F)
data.dmodel <- as.data.frame(predict(data.dmodel, data))
set.seed(100)
train.index <- sample(nrow(data.dmodel), nrow(data.dmodel) * .85) #let keep 85% of data for training
###############################
# split data into train and test
set.seed(195)
train.index <- sample(nrow(df_transformed), nrow(df_transformed) * .8) #let keep 80% of data for training
require(tidyr)
library(stringr)
library(dplyr)
itineraries <- read.csv('itineraries_subset.csv')
setwd("~/Google Drive/MIS720/MIS720 Project")
# drop all NA values
data <- na.omit(itineraries)
# define data types
data$legId <- as.character(data$legId)
data$searchDate <- strptime(data$searchDate, "%Y-%m-%d")
data$flightDate <- strptime(data$flightDate, "%Y-%m-%d")
data$segmentsDepartureTimeRaw <- strptime(data$segmentsDepartureTimeRaw, "%Y-%m-%dT%H:%M")
# group departure time into morning (6am-11:59am), afternoon (12:00pm-5:59pm), night (6:00pm-11:59pm), red-eye (12:00am-5:59am)
seg_cols = c("dept_date", "dept_time")
data <-  cbind(data, str_split_fixed(data$segmentsDepartureTimeRaw, " ", 2))
data <- data %>% rename(dept_time = `2`)
data <- data %>% separate(dept_time, into=c("dept_hrs", "dept_mins"))
data$dept_hrs <- as.integer(data$dept_hrs)
data <- data %>% mutate(dept_time_group = case_when(dept_hrs < 6 ~ "red-eye",
dept_hrs < 12 ~ "morning",
dept_hrs < 18 ~ "afternoon",
dept_hrs < 24 ~ "night"))
# split Airline Name column by legs
data <- data %>% separate(segmentsAirlineName , into = c("airline_first_leg", "airline_second_leg"))
# split Airline Equipment column by legs
data <- data %>% separate(segmentsEquipmentDescription,  into=c('aircraft_type_first_leg'))
# split Cabin code columb by legs
data <- data %>% separate(segmentsCabinCode,  into=c('cabin_first_leg'))
# remove leading PT and transform hours to minutes
data$travelDuration <- gsub("^.{0,2}", "", data$travelDuration)
# split columns
data <- data %>% separate(travelDuration, into = c("travelDuration_hours", "travelDuration_mins"), sep="H")
# remove last character M
data$travelDuration_mins <- gsub("M","", as.character(data$travelDuration_mins))
# covert hours into minutes
data$travelDuration_hours_to_mins <- as.integer(data$travelDuration_hours) * 60
data$travelDuration_minutes <- data$travelDuration_hours_to_mins + as.integer(data$travelDuration_mins)
data_cleaned <- subset(data, select=-c(travelDuration_hours_to_mins, travelDuration_mins, travelDuration_hours, `1`, dept_hrs, dept_mins, segmentsDepartureTimeRaw, segmentsArrivalTimeRaw))
# descriptive statistics
summary(data_cleaned)
colnames(data_cleaned)
dim(data_cleaned)
library(caret) #function of function
library(MASS)
library(e1071)
library(psych)
library(corrplot) #viz correlation matrices
library(corrgram) #viz correlation matrices
library(ISLR)
library(rpart)
# drop irrelevant columns
df <- subset(data_cleaned, select=-c(baseFare, X, legId, searchDate, flightDate, fareBasisCode))
df <- subset(df, select=-c(segmentsDepartureTimeEpochSeconds, segmentsArrivalTimeEpochSeconds, segmentsArrivalAirportCode, segmentsDepartureAirportCode, segmentsAirlineCode, segmentsDurationInSeconds, segmentsDistance)) # remove date/time columns
df$travelDuration_minutes <- as.integer(df$travelDuration_minutes)
df<- na.omit(df)
str(df)
dim(df)
# identifying skewed variables
skewValues <- apply(df[sapply(df,is.numeric)], 2, skew)
skewSE <- sqrt(6/nrow(df))
##Visualize data distributions of variables with high skew identified above
multi.hist(df[sapply(df,is.numeric)][,abs(skewValues)/skewSE > 2]) # totalFare and travelDuration are left skewed
par(mfrow=c(1,1)) #reset plot dispaly to 1
# remove outliers from totalFare and travelDuration_minutes
quartiles <- quantile(df$travelDuration_minutes, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(df$travelDuration_minutes)
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR
data_no_outlier <- subset(df, df$travelDuration_minutes > Lower & df$travelDuration_minutes < Upper)
quartiles <- quantile(data_no_outlier$totalFare, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(df$totalFare)
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR
data_no_outlier <- subset(df, df$totalFare > Lower & df$totalFare< Upper)
dim(data_no_outlier)
skewValues <- apply(data_no_outlier[sapply(data_no_outlier,is.numeric)], 2, skew)
skewSE <- sqrt(6/nrow(data_no_outlier))
multi.hist(data_no_outlier[sapply(data_no_outlier,is.numeric)][,abs(skewValues)/skewSE > 2])
df <- data_no_outlier
#identify correlated predictors
df_pred <- subset(df, select=-c(totalFare))
df.cor <- cor(df_pred[sapply(df_pred,is.numeric)]) #omitting DV
round(df.cor,2)
corrplot(df.cor, order="hclust") #visualize and cluster by high correlation
#some highly correlated predictors could hurt prediction accuracy
corrgram(df.cor, upper.panel = panel.cor,
lower.panel = panel.pie)
#remove zero or near zero variance, reducing cut off
df.novar<- nearZeroVar(df, freqCut=5/1)
df_processed <- df[,-c(df.novar)]
boxplot(x = as.list(df_processed[sapply(df_processed,is.numeric)]))
boxplot(x = as.list(df_processed[sapply(df_processed,is.numeric)][,-2]))
#using caret create a model of the dummy variables, with Full rank so N factor levels-1 new predictors
df_d.model <- dummyVars( ~ ., data=df_processed, fullRank=T)
df.d <- as.data.frame(predict(df_d.model, df_processed))
str(df.d)
# standardize variables
preProcValues <- preProcess(df.d, method = c("center", "scale"))
df_transformed <- predict(preProcValues, df.d)
###############################
# split data into train and test
set.seed(195)
train.index <- sample(nrow(df_transformed), nrow(df_transformed) * .8) #let keep 80% of data for training
df.train <- df_transformed[train.index,] #model with this
df.test <- df_transformed[-train.index,] #we don't touch this while training
## MODELING
## Linear Regression
# first set parameters of resampling and training
# uses 5-fold cross validation, regression performance measures, and saves predictions for all cross validations
ctrl <- trainControl(method = "cv", number=5)
str(df_transformed)
dim(df_transformed)
dim(Df)
df(df)
dim(df)
colnames(df)
###############################
# split data into train and test
set.seed(195)
train.index <- sample(nrow(df_transformed), nrow(df_transformed) * .8) #let keep 80% of data for training
df.train <- df_transformed[train.index,] #model with this
df.test <- df_transformed[-train.index,] #we don't touch this while training
## MODELING
## Linear Regression
# first set parameters of resampling and training
# uses 5-fold cross validation, regression performance measures, and saves predictions for all cross validations
ctrl <- trainControl(method = "cv", number=5)
#lets fit our first linear regression model
set.seed(195) #ALWAYS USE same SEED ACROSS trains to ensure identical cv folds
c.lm <- train(totalFare ~ ., data= df.train, method = "lm", trControl=ctrl)
#Review cross-validation performance
c.lm
#we can review what variables were most important
varImp(c.lm)
#we can also review the final trained model just like if we used lm()
summary(c.lm)
#review residuals and actual observations against predictions
#should review interesting models using graphs and summary
par(mfrow=c(1,2))
plot(df.train$totalFare ~ predict(c.lm), xlab="predict", ylab="actual")
colnames(df.train)
colnames(c.lm)
plot(df.train$totalFare ~ predict(c.lm), xlab="predict", ylab="actual")
plot(resid(c.lm) ~ predict(c.lm), xlab="predict", ylab="resid")
c.lm
predict(c.lm)
?predict
colnames(c.lm)
varImp(c.lm)
predict(c.lm, df.train)
#review residuals and actual observations against predictions
#should review interesting models using graphs and summary
par(mfrow=c(1,2))
plot(df.train$totalFare ~ predict(c.lm, df.train), xlab="predict", ylab="actual")
plot(resid(c.lm) ~ predict(c.lm), xlab="predict", ylab="resid")
plot(resid(c.lm) ~ predict(c.lm, df.train), xlab="predict", ylab="resid")
#lets fit a model that preprocess predictors on each resample fold based on our earlier findings
set.seed(195) #ALWAYS USE same SEED ACROSS trains to ensure identical cv folds
c.lm.pp <- train(totalFare ~ ., data= df.train,
preProcess=c("BoxCox", "scale", "center"),
method = "lm", trControl=ctrl)
c.lm.pp
#let create a model removing correlated predictors
set.seed(195) #ALWAYS USE same SEED ACROSS trains to ensure identical cv folds
c.lm.cor <- train(totalFare ~ ., data= df.train,
preProcess=c("corr"),
method = "lm", trControl=ctrl)
c.lm.cor
##########TEST DATA EVALUATION
#creating function called testPerformance to run against each model
#to measure test performance
testPerformance <- function(model) {
#evaluate  performance
postResample(predict(model, df.test), df.test$Balance)
}
testPerformance()
#apply the test performance function against each of the models in the list
lapply(c.models,testPerformance)
#evaluate  performance
postResample(predict(c.lm, df.test), df.test$Balance)
testPerformance
#evaluate  performance
postResample(predict(model, df.test), df.test$Balance)
##########TEST DATA EVALUATION
#creating function called testPerformance to run against each model
#to measure test performance
testPerformance <- function(model) {
#evaluate  performance
postResample(predict(model, df.test), df.test$Balance)
}
testPerformance(c.lm)
#evaluate  performance
postResample(predict(model, df.test), df.test$totalFare)
##########TEST DATA EVALUATION
#creating function called testPerformance to run against each model
#to measure test performance
testPerformance <- function(model) {
#evaluate  performance
postResample(predict(model, df.test), df.test$totalFare)
}
testPerformance(c.lm)
#set values of k to search through, K 1 to 15
k.grid <- expand.grid(k=1:10)
set.seed(195) #ALWAYS USE same SEED ACROSS trains to ensure identical cv folds
c.knn <- train(totalFare ~ ., data= df.train, method = "knn",
tuneGrid=k.grid, trControl=ctrl)
c.knn
getTrainPerf(c.knn)
##COMPARE MODEL PERFORMANCE
#let's compare all of the model resampling performance
#first lets put all trained models in a list object
c.models<- list("LM Base"=c.lm, "KNN Base" = c.knn.pp)
c.knn
getTrainPerf(c.knn)
varImp(c.knn)
#we can plot parameter performance
plot(c.knn)
##COMPARE MODEL PERFORMANCE
#let's compare all of the model resampling performance
#first lets put all trained models in a list object
c.models<- list("LM Base"=c.lm, "KNN Base" = c.knn)
fare.resamples<- resamples(c.models)
summary(fare.resamples)
#plot performances
bwplot(fare.resamples, metric="RMSE")
bwplot(fare.resamples, metric="Rsquared")
#plot performances
bwplot(fare.resamples, metric="RMSE")
#evalaute differences in model and see if their are statistical sig differnces
fare.diff <- diff(fare.resamples)
summary(fare.diff) #lower diagonal is p-value of differences
testPerformance(c.lm)
#apply the test performance function against each of the models in the list
lapply(c.models,testPerformance)
colnames(df.d)
colnames(df_processed)
for i in colnames(df_processed): i
colnames(df_processed)
View(df_processed)
#apply the test performance function against each of the models in the list
lapply(c.models,testPerformance)
setwd("~/Desktop/PersonalProjects_Github/BDA_600_Project")
data <- read.csv('cumulative_returns.csv')
head(data)
typeof(data)
data['X']
data['X'] = as.Date(data['X'], tryFormat = "%Y-%m-%d")
data['X'] = as.Date(data$X, tryFormat = "%Y-%m-%d")
data
head(data)
typeof(data$X)
data$X = as.Date(data$X, tryFormat = "%Y-%m-%d")
head(data)
plot(data$X, data$layoff_group, type = 'l', xlab = 'Time', ylab = 'Return', main = 'Cumulative Return of Layoff Group 2020 - 2022')
plot(data$X, data$no_layoff_group, type = 'l', xlab = 'Time', ylab = 'Return', main = 'Cumulative Return of Layoff Group 2020 - 2022')
dates = data$X
logx2 = log(no_layoff_data)
no_layoff_data = data$no_layoff_group
logx1 = log(layoff_data)
dates = data$X
layoff_data = data$layoff_group
no_layoff_data = data$no_layoff_group
logx1 = log(layoff_data)
logx2 = log(no_layoff_data)
plot(dates, logx1)
plot(dates, logx1, type='l')
plot(dates, logx2, type='l')
dlogx1 = diff(logx1)
plot(dates, dlogx1, type='l')
dlogx1 = diff(log(layoff_data))
logx2 = log(no_layoff_data)
plot(dates, dlogx1, type='l')
length(dlogx1)
length(dates)
dates = as.Date(data$X[2:n], tryFormat = "%Y-%m-%d")
n = dim(data)[1]
n
dates = as.Date(data$X[2:n], tryFormat = "%Y-%m-%d")
layoff_data = data$layoff_group
no_layoff_data = data$no_layoff_group
# log to make stationary
logx1 = log(layoff_data)
dlogx1 = diff(log(layoff_data))
logx2 = log(no_layoff_data)
plot(dates, dlogx1, type='l')
install.packages("fBasics")
install.packages("lmtest")
library(fBasics) # Load the package fBasics.
library(tseries)
adf.test(dlogx1)
install.packages('tseriest')
install.packages('tseriest)
library(fBasics) # Load the package fBasics.
library(tseries)
library(astsa)
library(lmtest)
library(TSA)
data <- read.csv('cumulative_returns.csv')
install.packages('tseries't')
install.packages('tseries')
library(fBasics) # Load the package fBasics.
library(tseries)
library(astsa)
install.packages('astsa')
library(astsa)
library(lmtest)
library(TSA)
install.packages('TSA')
library(TSA)
adf.test(dlogx1)
acf(dlogx1)
pacf(dlogx1)
acf(dlogx1)
pacf(dlogx1)
acf(logx1)
pacf(logx1)
adf.test(logx1) # P < .05, REGECT H0. data is stationary
adf.test(dlogx1) # P < .05, REGECT H0. data is stationary
acf(logx1)
acf(dlogx1)
pacf(dlogx1)
eacf(dlogx1)
data <- read.csv('cumulative_weekly_returns.csv')
data$X = as.Date(data$X, tryFormat = "%Y-%m-%d")
head(data)
plot(data$X, data$layoff_group, type = 'l', xlab = 'Time', ylab = 'Return', main = 'Cumulative Return of Layoff Group 2020 - 2022')
plot(data$X, data$no_layoff_group, type = 'l', xlab = 'Time', ylab = 'Return', main = 'Cumulative Return of Layoff Group 2020 - 2022')
n = dim(data)[1]
dates = as.Date(data$X[2:n], tryFormat = "%Y-%m-%d")
layoff_data = data$layoff_group
no_layoff_data = data$no_layoff_group
# log to make stationary
logx1 = log(layoff_data)
dlogx1 = diff(log(layoff_data))
plot(dates, dlogx1, type='l')
adf.test(dlogx1) # P < .05, REGECT H0. data is stationary
acf(dlogx1)
pacf(dlogx1)
acf(dlogx1)
eacf(dlogx1)
plot(dates, dlogx1, type='l')
plot(data$X, data$layoff_group, type = 'l', xlab = 'Time', ylab = 'Return', main = 'Cumulative Return of Layoff Group 2020 - 2022')
